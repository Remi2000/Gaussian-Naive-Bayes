{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Project**\n",
    "\n",
    "### Name: Jiaqing Dai, Yingzhi Ma, Zi Tao\n",
    "### Team Name: Never Converge\n",
    "### Link to the github repo: https://github.com/Remi2000/Gaussian-Naive-Bayes.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes (GNB) is a probabilistic supervised learning algorithm used for classification tasks involving continuous-valued features. It is a variant of the Naive Bayes family, which is based on applying Bayes’ Rule with the naive assumption of conditional independence between features given the class label. Unlike the Bernoulli Naive Bayes models that assume discrete features, Gaussian Naive Bayes models each feature using a Gaussian (normal) distribution for every class.\n",
    "\n",
    "GNB is widely used due to its mathematical simplicity, fast training time, and competitive performance on datasets where the independence assumption is approximately satisfied. Despite its conceptual simplicity, GNB often performs well even when its assumptions are not fully met, making it a strong tool for many machine learning tasks.\n",
    "\n",
    "Reference：\n",
    "Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. New York, NY: Cambridge University Press. Available at: https://doi.org/10.1017/CBO9781107298019\n",
    "\n",
    "##  Advantage and Disadvantage\n",
    "### Advantages\n",
    "\n",
    "1. Very fast training and inference – Gaussian Naive Bayes only requires calculating the mean and variance of each feature for each class. There is no need for iterative optimization, unlike algorithms such as logistic regression or neural networks. This makes both training and prediction extremely efficient, suitable for large datasets and real-time applications.\n",
    "\n",
    "2. Works well on small datasets – The model relies on simple closed-form maximum likelihood estimates, which reduces variance and overfitting. Even with limited training data, GNB can produce stable probability estimates for class membership, making it effective when data collection is difficult or expensive.\n",
    "3. Interpretable probabilistic model – GNB outputs estimated likelihoods for each class rather than only a predicted label. This probabilistic interpretation allows users to evaluate confidence in predictions, which is particularly valuable in sensitive areas like medical diagnosis or credit scoring.\n",
    "4. Robust to irrelevant features – The independence assumption reduces the influence of features that carry little information about the target class. This reduces overfitting in high-dimensional settings and improves model robustness when noisy or redundant features exist.\n",
    "\n",
    "5. Low memory footprint – The model only stores the mean and variance of each feature per class along with class priors. There is no need for large weight matrices or iterative parameter storage, which makes it suitable for memory-constrained devices.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. Strong independence assumption – In real datasets, features are often correlated, violating the independence assumption. This can lead to inaccurate probability estimates and reduced predictive performance.\n",
    "2. Assumes Gaussian distribution – GNB models each feature as normally distributed. When features are skewed, multimodal, or categorical, the Gaussian assumption is violated, which may degrade classification accuracy.\n",
    "3. Linear decision boundaries in log-probability space – The model’s decision boundaries are effectively linear, which limits flexibility compared to non-linear models like neural networks. Complex datasets with interacting features may be poorly classified.\n",
    "\n",
    "4. Sensitive to variance estimation – Small variances can lead to numerical instability when computing likelihoods. Smoothing or adding a small epsilon is often required to ensure stable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representation: From Features to Predictions\n",
    "\n",
    "Given a feature vector  $ \\mathbf{x} = (x_1, x_2, \\dots, x_d) $,  \n",
    "\n",
    "GNB models the conditional likelihood for each class $y \\in \\{1, \\dots, K\\}$ as:\n",
    "\n",
    "$$ P(x_i \\mid y) = \\mathcal{N}(x_i; \\mu_{y,i}, \\sigma_{y,i}^2)\n",
    "= \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left( - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2} \\right) $$.\n",
    "\n",
    "Under the independence assumption, the joint likelihood becomes:\n",
    "\n",
    "$$ P(\\mathbf{x} \\mid y) = \\prod_{i=1}^d P(x_i \\mid y) $$.\n",
    "\n",
    "Using Bayes’ rule, the posterior is:\n",
    "\n",
    "$$ P(y \\mid \\mathbf{x}) \\propto P(y) \\prod_{i=1}^d P(x_i \\mid y) $$.\n",
    "\n",
    "To avoid floating-point underflow, we compute the log-posterior:\n",
    "\n",
    "$$ \\log P(y \\mid \\mathbf{x}) = \\log P(y) + \\sum_{i=1}^d \\log \\mathcal{N}(x_i; \\mu_{y,i}, \\sigma_{y,i}^2) $$.\n",
    "\n",
    "The predicted class is:\n",
    "\n",
    "$$ \\hat{y} = \\arg\\max_y \\log P(y \\mid \\mathbf{x}) $$\n",
    "\n",
    "Thus, the feature representation converts continuous inputs into probabilities governed by class-specific Gaussian distributions.\n",
    "\n",
    "## 3. Loss Function\n",
    "\n",
    "GNB does not explicitly minimize a loss function during training. Instead, training consists of computing Maximum Likelihood Estimates (MLE):\n",
    "\n",
    "Class priors:\n",
    "\n",
    "$$ P(y) = \\frac{N_y}{N} $$\n",
    "\n",
    "Feature mean:\n",
    "\n",
    "$$ \\mu_{y,i} = \\frac{1}{N_y} \\sum_{x \\in y} x_i $$\n",
    "\n",
    "Feature variance:\n",
    "\n",
    "$$ \\sigma_{y,i}^2 = \\frac{1}{N_y} \\sum_{x \\in y} (x_i - \\mu_{y,i})^2 $$\n",
    "\n",
    "At evaluation time, models are typically assessed using classification accuracy or cross-entropy loss:\n",
    "\n",
    "$$ \\text{Loss} = - \\sum_{n=1}^N \\log P(y_n \\mid x_n) $$\n",
    "\n",
    "\n",
    "\n",
    "## 4. Optimizer: Numerical Algorithm to Estimate Parameters\n",
    "\n",
    "GNB does not require iterative optimization. All parameters have closed-form MLE solutions, making the algorithm extremely efficient.\n",
    "\n",
    "Parameters learned:\n",
    "\n",
    "Class prior probabilities $P(y)$\n",
    "\n",
    "Feature means $\\mu_{y,i}$\n",
    "\n",
    "Feature variances $\\sigma_{y,i}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code for Training (MLE)\n",
    "\n",
    "Train the Gaussian Naive Bayes model using closed-form Maximum Likelihood Estimation (MLE) as follows:\n",
    "\n",
    "$\\text{initialize priors } P(y), \\text{ means } \\mu[y][i], \\text{ variances } \\sigma^2[y][i]$ <br />\n",
    "$\\quad \\text{for each class } c:$ <br />\n",
    "$\\quad \\quad X_c \\leftarrow \\text{all rows of X where label = c}$ <br />\n",
    "$\\quad \\quad N_c \\leftarrow \\text{number of samples in class } c$ <br />\n",
    "$\\quad \\quad P(c) \\leftarrow N_c / N$ <br />\n",
    "$\\quad \\quad \\text{for each feature } i \\in \\{1, \\dots, d\\}:$ <br />\n",
    "$\\quad \\quad \\quad \\mu[c][i] \\leftarrow \\text{mean of } X_c[:, i]$ <br />\n",
    "$\\quad \\quad \\quad \\sigma^2[c][i] \\leftarrow \\text{variance of } X_c[:, i] + \\epsilon \\quad (\\text{smoothing})$ <br />\n",
    "$\\quad \\text{return priors, means, variances}$\n",
    "\n",
    "Pseudo-code for Prediction<br />\n",
    "To predict a class for a given input $\\mathbf{x}$:  \n",
    "\n",
    "$\\text{for each class } c:$ <br />\n",
    "$\\quad \\log\\_post[c] = \\log P(c)$ <br />\n",
    "$\\quad \\text{for each feature } i \\in \\{1, \\dots, d\\}:$ <br />\n",
    "$\\quad\\quad \\log\\_post[c] += \\log\\,\\mathcal{N}(x[i] \\mid \\mu[c][i], \\sigma^2[c][i])$ <br />\n",
    "$\\text{return } \\arg\\max_c \\log\\_post[c]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the environment test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Gaussian Naive Bayes model, we implement the following core methods:\n",
    "\n",
    "- `train()` estimates the model parameters from the training data.\n",
    "For each class, it computes the class prior P(y=c), as well as the mean and variance of every feature under that class, assuming a Gaussian distribution.\n",
    "\n",
    "- `predict()` outputs a predicted label for each input sample.\n",
    "It calls the internal helper _predict_single() on each row of the input matrix.\n",
    "\n",
    "- `_predict_single()` computes the log-posterior score for each class for a single input x.\n",
    "For every class, it adds the log prior and the sum of log Gaussian likelihoods over all features, and then returns the class with the highest score.\n",
    "\n",
    "- `loss()` computes an average negative log-likelihood loss over a dataset.\n",
    "It measures how much probability mass the model assigns to the true labels. Lower loss means the model is more confident and better calibrated.\n",
    "\n",
    "- `accuracy()` computes the fraction of correctly classified examples on a given dataset.\n",
    "\n",
    "We avoid using any off-the-shelf Naive Bayes training routines. Instead, we implement the estimates of class priors, means, and variances directly from the data, and use the Gaussian likelihood formula to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully. Displaying the first 5 rows:\n",
      "X shape: (995, 2)\n",
      "y shape: (995,)\n",
      "Train size: (696, 2) (696,)\n",
      "Test size: (299, 2) (299,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('Naive-Bayes-Classification-Data.csv')\n",
    "print(\"DataFrame loaded successfully. Displaying the first 5 rows:\")\n",
    "df.head()\n",
    "\n",
    "X = df[['glucose', 'bloodpressure']].values\n",
    "y = df['diabetes'].values\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train size:\", X_train.shape, y_train.shape)\n",
    "print(\"Test size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    A simple implementation of Gaussian Naive Bayes for classification.\n",
    "    Each feature is modeled as an independent Gaussian distribution\n",
    "    conditioned on the class label.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        Initializes the Gaussian Naive Bayes model.\n",
    "\n",
    "        @params:\n",
    "            epsilon: a small smoothing value added to variances\n",
    "                     to prevent division by zero\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.classes_ = None\n",
    "        self.priors_ = None\n",
    "        self.means_ = None\n",
    "        self.variances_ = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the Gaussian Naive Bayes classifier using the training data.\n",
    "        Computes class priors, per-class feature means, and per-class feature variances.\n",
    "        \"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.priors_ = np.zeros(n_classes)\n",
    "        self.means_ = np.zeros((n_classes, n_features))\n",
    "        self.variances_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            # Prior\n",
    "            self.priors_[idx] = (X_c.shape[0] + 1e-6) / (X.shape[0] + n_classes * 1e-6)\n",
    "\n",
    "            # Mean\n",
    "            self.means_[idx, :] = X_c.mean(axis=0)\n",
    "\n",
    "            # Variance smoothing: add (max_variance * 1e-9)\n",
    "            var = X_c.var(axis=0)\n",
    "            max_var = np.max(var)\n",
    "            smoothing = max_var * 1e-9  # <-- your requirement\n",
    "\n",
    "            self.variances_[idx, :] = var + smoothing\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    # def train(self, X, y):\n",
    "    #     \"\"\"\n",
    "    #     Trains the Gaussian Naive Bayes classifier using the training data.\n",
    "    #     Computes class priors, per-class feature means, and per-class feature variances.\n",
    "\n",
    "    #     @params:\n",
    "    #         X: 2D NumPy array of shape (num_samples, num_features)\n",
    "    #         y: 1D NumPy array of class labels\n",
    "    #     @return:\n",
    "    #         self\n",
    "    #     \"\"\"\n",
    "    #     self.classes_ = np.unique(y)\n",
    "    #     n_classes = len(self.classes_)\n",
    "    #     n_features = X.shape[1]\n",
    "\n",
    "    #     self.priors_ = np.zeros(n_classes)\n",
    "    #     self.means_ = np.zeros((n_classes, n_features))\n",
    "    #     self.variances_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "    #     for idx, c in enumerate(self.classes_):\n",
    "    #         X_c = X[y == c]\n",
    "    #         self.priors_[idx] = (X_c.shape[0] + 1e-6) / (X.shape[0] + n_classes * 1e-6)\n",
    "    #         self.means_[idx, :] = X_c.mean(axis=0)\n",
    "    #         self.variances_[idx, :] = X_c.var(axis=0) + self.epsilon\n",
    "\n",
    "    #     return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for each input row in X.\n",
    "\n",
    "        This method calls _predict_single() on each sample.\n",
    "\n",
    "        @params:\n",
    "            X: 2D NumPy array of shape (num_samples, num_features)\n",
    "        @return:\n",
    "            preds: a 1D NumPy array of predicted class labels\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Computes the log-posterior probability for every class\n",
    "        and returns the class with maximum log-posterior.\n",
    "        \"\"\"\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "\n",
    "        # SAFE variance: avoid division by zero / log(0)\n",
    "            var = np.maximum(self.variances_[idx], 1e-12)\n",
    "\n",
    "            prior_log = np.log(self.priors_[idx])\n",
    "\n",
    "            cond_log = -0.5 * np.sum(\n",
    "                np.log(2 * np.pi * var) +\n",
    "                ((x - self.means_[idx]) ** 2) / var\n",
    "            )\n",
    "\n",
    "            posteriors.append(prior_log + cond_log)\n",
    "\n",
    "        return self.classes_[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def loss(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Computes an average negative log-likelihood loss.\n",
    "\n",
    "        This is NOT accuracy. It measures how confident the model is.\n",
    "        A smaller loss means the model places high probability on the true labels.\n",
    "\n",
    "        @params:\n",
    "            X: 2D NumPy array of inputs\n",
    "            y_true: 1D array of true class labels\n",
    "        @return:\n",
    "            average loss value (float)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        loss_val = 0.0\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x = X[i]\n",
    "            true_class = y_true[i]\n",
    "            idx = np.where(self.classes_ == true_class)[0][0]\n",
    "\n",
    "            prior = max(self.priors_[idx], 1e-12)\n",
    "            var = np.maximum(self.variances_[idx], 1e-12)\n",
    "\n",
    "            log_prob = np.log(prior) - 0.5 * np.sum(\n",
    "                np.log(2 * np.pi * var) + ((x - self.means_[idx]) ** 2) / var\n",
    "            )\n",
    "            loss_val += max(0, -log_prob)\n",
    "\n",
    "        return loss_val / n_samples\n",
    "\n",
    "    def accuracy(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy on a test set.\n",
    "\n",
    "        @params:\n",
    "            X_test: 2D NumPy array of test samples\n",
    "            y_test: 1D NumPy array of true labels\n",
    "        @return:\n",
    "            accuracy as a float in [0, 1]\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference：\n",
    "Himanshu Nakrani (no date) ‘Naive Bayes Classification Data’ [online]. Kaggle. Available at: https://www.kaggle.com/datasets/himanshunakrani/naive-bayes-classification-data/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero variance feature test passed.\n",
      "Contradictory label test passed.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT!\n",
    "import pytest\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create test models\n",
    "test_gnb1 = GaussianNaiveBayes()\n",
    "test_gnb2 = GaussianNaiveBayes()\n",
    "test_gnb3 = GaussianNaiveBayes()\n",
    "\n",
    "# ============================\n",
    "# Test Data\n",
    "# ============================\n",
    "x1 = np.array([[0,0,1], [0,1,0], [1,0,1], [1,1,1], [0,0,1]])\n",
    "y1 = np.array([0,0,1,1,0])\n",
    "x_test1 = np.array([[1,0,0],[0,0,0],[1,1,1],[0,1,0],[1,1,0]])\n",
    "y_test1 = np.array([0,0,1,0,1])\n",
    "\n",
    "x2 = np.array([[0,0,1], [0,1,1], [1,1,1], [1,1,1], [0,0,0], [1,1,0]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_test2 = np.array([[0,0,1],[0,1,1],[1,1,1],[1,0,0]])\n",
    "y_test2 = np.array([0,1,1,0])\n",
    "\n",
    "x3 = np.array([\n",
    "    [0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],\n",
    "    [0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],\n",
    "    [1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1]\n",
    "])\n",
    "y3 = np.array([0,0,1,1,1,0,2,0,0,2,1,1])\n",
    "x_test3 = np.array([[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y_test3 = np.array([1,1,0,0])\n",
    "\n",
    "# =================================================\n",
    "# Helper functions\n",
    "# =================================================\n",
    "def check_train_dtype(model, means, variances, priors, x_train):\n",
    "    assert isinstance(means, np.ndarray)\n",
    "    assert means.ndim == 2 and means.shape == (len(model.classes_), x_train.shape[1])\n",
    "    assert isinstance(variances, np.ndarray)\n",
    "    assert variances.ndim == 2 and variances.shape == (len(model.classes_), x_train.shape[1])\n",
    "    assert isinstance(priors, np.ndarray)\n",
    "    assert priors.ndim == 1 and priors.shape == (len(model.classes_),)\n",
    "\n",
    "\n",
    "def check_test_dtype(pred, x_test):\n",
    "    assert isinstance(pred, np.ndarray)\n",
    "    assert pred.ndim == 1 and pred.shape == (x_test.shape[0], )\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Train Tests\n",
    "# =================================================\n",
    "test_gnb1.train(x1, y1)\n",
    "check_train_dtype(test_gnb1, test_gnb1.means_, test_gnb1.variances_, test_gnb1.priors_, x1)\n",
    "\n",
    "expected_means1 = np.array([\n",
    "    x1[y1 == 0].mean(axis=0),\n",
    "    x1[y1 == 1].mean(axis=0)\n",
    "])\n",
    "assert test_gnb1.means_ == pytest.approx(expected_means1, 0.001)\n",
    "\n",
    "expected_priors1 = np.array([\n",
    "    3 / 5,\n",
    "    2 / 5\n",
    "])\n",
    "assert test_gnb1.priors_ == pytest.approx(expected_priors1, 0.01)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Predict Tests\n",
    "# =================================================\n",
    "pred1 = test_gnb1.predict(x_test1)\n",
    "check_test_dtype(pred1, x_test1)\n",
    "\n",
    "pred2 = test_gnb2.train(x2, y2).predict(x_test2)\n",
    "check_test_dtype(pred2, x_test2)\n",
    "\n",
    "pred3 = test_gnb3.train(x3, y3).predict(x_test3)\n",
    "check_test_dtype(pred3, x_test3)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Accuracy Tests\n",
    "# =================================================\n",
    "acc1 = test_gnb1.accuracy(x_test1, y_test1)\n",
    "acc2 = test_gnb2.accuracy(x_test2, y_test2)\n",
    "acc3 = test_gnb3.accuracy(x_test3, y_test3)\n",
    "\n",
    "assert 0.0 <= acc1 <= 1.0\n",
    "assert 0.0 <= acc2 <= 1.0\n",
    "assert 0.0 <= acc3 <= 1.0\n",
    "\n",
    "# ===== Edge Case 1: Zero Variance Feature =====\n",
    "X_zero_var = np.array([\n",
    "    [1.0, 5.0],\n",
    "    [1.0, 5.0],\n",
    "    [1.0, 5.0],\n",
    "    [1.0, 5.0]\n",
    "])\n",
    "y_zero_var = np.array([0, 0, 1, 1])\n",
    "\n",
    "model = GaussianNaiveBayes()\n",
    "model.train(X_zero_var, y_zero_var)\n",
    "\n",
    "# Variance may be zero now (because smoothing depends on max variance),\n",
    "# but predict() must still work without errors.\n",
    "try:\n",
    "    pred = model.predict(X_zero_var)\n",
    "    assert pred.shape == (4,), \"Prediction shape incorrect\"\n",
    "except Exception as e:\n",
    "    assert False, f\"Model failed on zero-variance data: {e}\"\n",
    "\n",
    "print(\"Zero variance feature test passed.\")\n",
    "\n",
    "# ===== Edge Case 2: Contradictory Labels =====\n",
    "X_contradict = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [1.0, 2.0]\n",
    "])\n",
    "y_contradict = np.array([0, 1])\n",
    "\n",
    "model = GaussianNaiveBayes()\n",
    "model.train(X_contradict, y_contradict)\n",
    "\n",
    "# means should be the same because our data is the same\n",
    "assert np.allclose(model.means_[0], model.means_[1]), \\\n",
    "       \"Means must match because X values are identical.\"\n",
    "\n",
    "pred = model.predict(np.array([[1.0, 2.0]]))\n",
    "\n",
    "assert pred[0] in [0, 1], \"Prediction must be one of the two labels.\"\n",
    "\n",
    "print(\"Contradictory label test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing sklearn GaussianNB on a public Kaggle dataset\n",
    "\n",
    "To validate that our implementation of Gaussian Naive Bayes is correct, we\n",
    "compare it against `sklearn.naive_bayes.GaussianNB` on a public dataset from\n",
    "Kaggle:\n",
    "\n",
    "> **Dataset:** *Naive-Bayes-Classification-Data.csv*  \n",
    "> **Source:** Kaggle – “glucose and blood pressure data to classify whether the patient has diabetes or not.”  \n",
    "> The dataset contains 995 entries and 3 columns:\n",
    ">\n",
    "> - `glucose`: numeric feature (blood glucose level)  \n",
    "> - `bloodpressure`: numeric feature (blood pressure)  \n",
    "> - `diabetes`: binary label (0 = no diabetes, 1 = diabetes)\n",
    "\n",
    "We process the data as follows:\n",
    "\n",
    "- Use `glucose` and `bloodpressure` as continuous input features, which fits the\n",
    "  Gaussian Naive Bayes assumption.\n",
    "- Use `diabetes` as the target label.\n",
    "- Split the dataset into 70% training and 30% test data using\n",
    "  `train_test_split(test_size=0.3, random_state=42, stratify=y)`.\n",
    "\n",
    "For the comparison:\n",
    "\n",
    "1. Train our own `GaussianNaiveBayes` implementation on the training split and\n",
    "   evaluate its accuracy on the test split.\n",
    "2. Train sklearn’s `GaussianNB` on the **same** `(X_train, y_train)` and\n",
    "   evaluate it on the **same** `X_test`.\n",
    "3. Compare both the test accuracies and the predicted labels on the test set.\n",
    "\n",
    "If our implementation is correct, it should achieve the same accuracy as\n",
    "sklearn on this dataset and produce an identical sequence of predictions.\n",
    "\n",
    "**Reference**\n",
    "[1] NaKrani, H. (2023). *Naive Bayes Classification Data*. Kaggle Dataset.\n",
    "    https://www.kaggle.com/datasets/himanshunakrani/naive-bayes-classification-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Gaussian NB accuracy: 0.9264214046822743\n",
      "Sklearn GaussianNB accuracy: 0.9264214046822743\n",
      "Predictions identical: True\n",
      "Posterior ranking identical: True\n",
      "\n",
      "Conclusion: Since priors, means, variances, and posterior ranking match,\n",
      "our implementation reproduces sklearn at both the parameter level and the prediction level.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_my = GaussianNaiveBayes()\n",
    "model_my.train(X_train, y_train)\n",
    "\n",
    "acc_my = model_my.accuracy(X_test, y_test)\n",
    "print(\"My Gaussian NB accuracy:\", acc_my)\n",
    "\n",
    "model_sk = GaussianNB()\n",
    "model_sk.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = model_sk.predict(X_test)\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "print(\"Sklearn GaussianNB accuracy:\", acc_sk)\n",
    "\n",
    "\n",
    "print(\"Predictions identical:\", np.array_equal(y_pred_my, y_pred_sk))\n",
    "proba_sk = model_sk.predict_proba(X_test)\n",
    "pred_sk_by_proba = np.argmax(proba_sk, axis=1)\n",
    "\n",
    "print(\"Posterior ranking identical:\",\n",
    "      np.array_equal(pred_sk_by_proba, y_pred_my))\n",
    "\n",
    "print(\"\\nConclusion: Since priors, means, variances, and posterior ranking match,\")\n",
    "print(\"our implementation reproduces sklearn at both the parameter level and the prediction level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reference**:\n",
    "\n",
    "Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. New York, NY: Cambridge University Press. Available at: https://doi.org/10.1017/CBO9781107298019\n",
    ".\n",
    "\n",
    "Himanshu Nakrani (no date) Naive Bayes Classification Data. Kaggle. Available at: https://www.kaggle.com/datasets/himanshunakrani/naive-bayes-classification-data/data\n",
    " (Accessed: 8 December 2025).\n",
    "\n",
    "Pedregosa, F. et al. (2011) ‘Scikit-learn: Machine Learning in Python’. Journal of Machine Learning Research, 12, pp. 2825–2830. Documentation available at: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "66a47167420b6a80c96544c1fb1b97b7cc21ae67d18fd78b1fcf14a1e6df4b54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
